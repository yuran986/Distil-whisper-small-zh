# Distil-Whisper-Small-ZH: åŸºäºçŸ¥è¯†è’¸é¦çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«æ¨¡å‹

<div align="center">

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—%20Transformers-4.35+-yellow.svg)](https://github.com/huggingface/transformers)

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

</div>

---

## English

### ğŸ“‹ Project Overview

This project implements **knowledge distillation** for the Whisper speech recognition model, specifically optimized for **Chinese speech recognition tasks**. Through teacher-student distillation, we successfully compressed the Whisper-small model while maintaining high performance, achieving:

- ğŸ¯ **39.09% reduction** in model parameters (from 230.54M to 140.41M)
- âš¡ **2.89x speedup** in inference time
- ğŸ“ˆ **3.16% improvement** in Character Error Rate (CER)

### ğŸ¯ Key Features

- **Model Compression**: Utilizing knowledge distillation to reduce model size without significant performance loss
- **Multi-dataset Training**: Trained on Common Voice, FLEURS, and AISHELL datasets for robust Chinese speech recognition
- **Adaptive Fine-tuning**: Employing AdaLoRA for efficient parameter-efficient fine-tuning
- **Simplified Chinese Output**: Direct output of Simplified Chinese without requiring Traditional-to-Simplified conversion
- **Punctuation Recognition**: Enhanced ability to recognize punctuation marks in speech

### ğŸ—‚ï¸ Datasets

#### Common Voice Dataset
An open-source project initiated by Mozilla to collect large-scale speech data across multiple languages.
- **Multi-language Support**: Over 70 languages including Chinese
- **Diversity**: Recordings from speakers of various backgrounds, ages, and genders
- **Open Source**: Freely available for research and development

#### FLEURS Dataset
A multilingual speech command dataset released by Meta AI, supporting cross-language speech recognition research.
- **Multi-language Support**: 102 languages
- **Standardized Commands**: Common voice commands translated across languages
- **High-quality Recordings**: Clear audio with minimal background noise

#### AISHELL Dataset
An open Chinese speech recognition dataset widely used in academic research and industrial applications.
- **Domain**: Chinese Mandarin speech
- **Quality**: Professional recordings with detailed annotations
- **Enhancement**: Punctuation added using Alibaba's punc_ct-transformer

### ğŸ—ï¸ Model Architecture

#### Knowledge Distillation (Teacher-Student)

The distillation process follows a Teacher-Student paradigm:

- **Teacher Model**: Whisper-small (12 encoder layers + 12 decoder layers)
- **Student Model**: Distil-Whisper-small (12 encoder layers + 2 decoder layers)
- **Initialization Strategy**: Maximum-margin layer copying from teacher to student
- **Training Strategy**: Freeze encoder, train only decoder to optimize efficiency

#### Loss Functions

- **Cross-Entropy Loss**: Measures prediction accuracy against ground truth
- **KL Divergence Loss**: Captures knowledge transfer from teacher to student
- **Combined Loss**: Weighted combination (0.8 Ã— CE + KL weight Ã— KL)

### ğŸš€ Quick Start

#### Environment Setup

1. **Install Dependencies**
   ```bash
   pip install -e .
   ```

2. **Configure Accelerate**
   ```bash
   accelerate config
   ```

3. **Hugging Face Authentication**
   ```bash
   git config --global credential.helper store
   huggingface-cli login
   ```

#### Pseudo-labeling

Generate pseudo-labels using the teacher model:

```bash
bash run_pseudo_labelling.sh
```

**Note**: Modify `model_name_or_path`, `dataset_name`, and other parameters in the script before running.

#### Model Initialization

Create and initialize the student model:

```bash
python create_student_model.py \
  --teacher_checkpoint "openai/whisper-small" \
  --encoder_layers 12 \
  --decoder_layers 2 \
  --save_dir "./distil-small-init"
```

The initialized student model will be saved to `./distil-small-init`.

#### Distillation Training

Execute distillation training script:

```bash
bash run_distillation.sh
```

The trained model will be saved to `./model`. In this project, the encoder is frozen and only the decoder is trained (configured via `freeze_encoder` parameter).

#### Model Evaluation

Evaluate the model performance:

```bash
bash run_eval_sf.sh
```

Evaluation metrics include:
- **CER** (Character Error Rate): Measures character-level recognition accuracy
- **RTF** (Real-Time Factor): Ratio of audio duration to processing time

### ğŸ“Š Experimental Results

#### Performance Comparison

| Model | Parameters | CER (avg) | Speed-up | Dataset |
|-------|-----------|-----------|----------|---------|
| Whisper-small | 230.54M | 17.43% | 1.0x | Common Voice + FLEURS + AISHELL |
| Distil-Whisper (before fine-tuning) | 140.41M | 27.53% | 2.26x | Common Voice + FLEURS + AISHELL |
| **Distil-Whisper-finetune** | **140.41M** | **17.11%** | **2.89x** | **Common Voice + FLEURS + AISHELL** |

#### Detailed Results by Dataset

**Whisper-small Performance:**

| Dataset | Size | CER | Validation Time (s) |
|---------|------|-----|---------------------|
| Common Voice | 10,626 | 21.51% | 9,767 |
| FLEURS | 945 | 16.87% | 1,718 |
| AISHELL | 7,176 | 13.92% | 4,990 |
| **Average** | - | **17.43%** | - |

**Distil-Whisper-finetune Performance:**

| Dataset | Size | CER | CER Change | Validation Time (s) | Time Reduction | Speed-up |
|---------|------|-----|------------|---------------------|----------------|----------|
| Common Voice | 10,626 | 18.73% | â†“12.93% | 2,511 | 74.29% | 2.89x |
| FLEURS | 945 | 24.54% | â†‘45.51% | 459 | 73.28% | 2.74x |
| AISHELL | 7,176 | 8.06% | â†“42.06% | 1,240 | 76.53% | 3.02x |
| **Average** | - | **17.11%** | **â†“3.16%** | - | **74.70%** | **2.89x** |

*Validation time measured on 2Ã—NVIDIA T4 GPUs with batch_size=64*

#### Key Findings

1. **Model Compression**: Successfully reduced model size by 39.09% (90M parameters)
2. **Inference Speed**: Achieved 2.89x speedup in inference time
3. **Recognition Accuracy**: Improved CER by 3.16% on average compared to the teacher model
4. **Language Adaptation**: Fine-tuned model shows superior performance on Chinese datasets compared to the original multilingual model
5. **Punctuation Recognition**: Enhanced ability to recognize and output punctuation marks
6. **Direct Simplified Output**: Eliminates the need for Traditional-to-Simplified Chinese conversion

### ğŸ“ˆ Training Process

#### Loss Curves

**Training Loss Progression:**
- **Initial Phase** (0-1000 steps): Rapid loss decrease as the model learns basic patterns
- **Middle Phase** (1000-2000 steps): Gradual loss reduction with slower convergence
- **Convergence Phase** (2000+ steps): Loss stabilizes around 0.2, indicating optimal training state

**Fine-tuning Loss:**
- Initial rapid decrease from ~0.55 to ~0.35
- Steady decline to ~0.2 over 10,000 steps
- Final convergence demonstrates effective adaptation to Chinese datasets

### ğŸ› ï¸ Additional Tools

- **`test_whisper.py`**: Perform inference on online datasets
- **`test_whisper_local.py`**: Perform inference on local WAV audio files
- **`count_params.py`**: Calculate model parameter count

### ğŸ“ Project Structure

```
Distil-whisper-small-zh/
â”œâ”€â”€ cer/                          # Character Error Rate evaluation
â”‚   â”œâ”€â”€ cer.py                    # CER metric implementation
â”‚   â””â”€â”€ data_utils.py             # Data utilities for CER calculation
â”œâ”€â”€ create_student_model.py       # Student model initialization
â”œâ”€â”€ run_distillation.py           # Main distillation training script
â”œâ”€â”€ run_distillation.sh           # Distillation training shell script
â”œâ”€â”€ run_pseudo_labelling.py       # Pseudo-label generation
â”œâ”€â”€ run_pseudo_labelling.sh       # Pseudo-labeling shell script
â”œâ”€â”€ run_eval.py                   # Model evaluation script
â”œâ”€â”€ run_eval_sf.sh                # Evaluation shell script
â”œâ”€â”€ test_whisper.py               # Online dataset inference
â”œâ”€â”€ test_whisper_local.py         # Local audio file inference
â”œâ”€â”€ count_params.py               # Parameter counting utility
â”œâ”€â”€ data_utils.py                 # Data processing utilities
â”œâ”€â”€ setup.py                      # Package setup configuration
â””â”€â”€ README.md                     # Project documentation
```

### ğŸ”¬ Methodology

#### Three-Phase Experimental Design

**Phase 1: Distillation**
- Extract pseudo-labels from teacher model
- Initialize student model with maximum-margin layer copying
- Train with combined CE and KL divergence loss
- Freeze encoder, train only decoder

**Phase 2: Fine-tuning**
- Apply AdaLoRA for parameter-efficient fine-tuning
- Train on combined Common Voice, FLEURS, and AISHELL datasets
- Enhance Chinese language adaptation
- Improve punctuation recognition

**Phase 3: Evaluation**
- Test on held-out datasets
- Compare with baseline models (Whisper-base, Whisper-tiny)
- Measure CER and RTF metrics
- Conduct qualitative analysis on real recordings

### ğŸ“– References

1. Radford, A., et al. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. arXiv preprint arXiv:2212.04356.
2. Sanh, V., et al. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.
3. Hugging Face. (2021). Distil-Whisper: Distilling OpenAI's Whisper for Faster, Smaller Models.

---

## ä¸­æ–‡

### ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®é’ˆå¯¹ Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹å®ç°äº†**çŸ¥è¯†è’¸é¦**æŠ€æœ¯ï¼Œä¸“é—¨ä¼˜åŒ–ç”¨äº**ä¸­æ–‡è¯­éŸ³è¯†åˆ«ä»»åŠ¡**ã€‚é€šè¿‡æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ–¹æ³•ï¼Œæˆ‘ä»¬æˆåŠŸå‹ç¼©äº† Whisper-small æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº†é«˜æ€§èƒ½è¡¨ç°ï¼Œå®ç°äº†ï¼š

- ğŸ¯ æ¨¡å‹å‚æ•°**å‡å°‘ 39.09%**ï¼ˆä» 230.54M é™è‡³ 140.41Mï¼‰
- âš¡ æ¨ç†é€Ÿåº¦**æå‡ 2.89 å€**
- ğŸ“ˆ å­—ç¬¦é”™è¯¯ç‡ï¼ˆCERï¼‰**é™ä½ 3.16%**

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **æ¨¡å‹å‹ç¼©**ï¼šåˆ©ç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯åœ¨ä¸æ˜¾è‘—æŸå¤±æ€§èƒ½çš„å‰æä¸‹å‡å°æ¨¡å‹è§„æ¨¡
- **å¤šæ•°æ®é›†è®­ç»ƒ**ï¼šåœ¨ Common Voiceã€FLEURS å’Œ AISHELL æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå®ç°é²æ£’çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«
- **è‡ªé€‚åº”å¾®è°ƒ**ï¼šé‡‡ç”¨ AdaLoRA è¿›è¡Œé«˜æ•ˆçš„å‚æ•°å¾®è°ƒ
- **ç®€ä½“ä¸­æ–‡è¾“å‡º**ï¼šç›´æ¥è¾“å‡ºç®€ä½“ä¸­æ–‡ï¼Œæ— éœ€ç¹ç®€è½¬æ¢
- **æ ‡ç‚¹ç¬¦å·è¯†åˆ«**ï¼šå¢å¼ºäº†å¯¹è¯­éŸ³ä¸­æ ‡ç‚¹ç¬¦å·çš„è¯†åˆ«èƒ½åŠ›

### ğŸ—‚ï¸ æ•°æ®é›†ä»‹ç»

#### Common Voice æ•°æ®é›†
Mozilla å‘èµ·çš„å¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨æ”¶é›†å¤šç§è¯­è¨€çš„å¤§è§„æ¨¡è¯­éŸ³æ•°æ®é›†ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒè¶…è¿‡ 70 ç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡
- **å¤šæ ·æ€§**ï¼šæ¥è‡ªä¸åŒèƒŒæ™¯ã€å¹´é¾„å’Œæ€§åˆ«çš„è¯´è¯è€…å½•éŸ³
- **å¼€æºæ€§**ï¼šå…è´¹æä¾›ç”¨äºç ”ç©¶å’Œå¼€å‘

#### FLEURS æ•°æ®é›†
Meta AI å‘å¸ƒçš„å¤šè¯­è¨€è¯­éŸ³å‘½ä»¤æ•°æ®é›†ï¼Œæ”¯æŒè·¨è¯­è¨€è¯­éŸ³è¯†åˆ«ç ”ç©¶ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒ 102 ç§è¯­è¨€
- **æ ‡å‡†åŒ–å‘½ä»¤**ï¼šè·¨è¯­è¨€ç¿»è¯‘çš„æ ‡å‡†è¯­éŸ³å‘½ä»¤
- **é«˜è´¨é‡å½•éŸ³**ï¼šæ¸…æ™°çš„éŸ³é¢‘ï¼ŒèƒŒæ™¯å™ªéŸ³å°‘

#### AISHELL æ•°æ®é›†
å¼€æ”¾çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«æ•°æ®é›†ï¼Œå¹¿æ³›åº”ç”¨äºå­¦æœ¯ç ”ç©¶å’Œå·¥ä¸šåº”ç”¨ã€‚
- **é¢†åŸŸ**ï¼šä¸­æ–‡æ™®é€šè¯è¯­éŸ³
- **è´¨é‡**ï¼šä¸“ä¸šå½•éŸ³ï¼Œå¸¦æœ‰è¯¦ç»†æ ‡æ³¨
- **å¢å¼ºå¤„ç†**ï¼šä½¿ç”¨é˜¿é‡Œè¾¾æ‘©é™¢çš„ punc_ct-transformer æ·»åŠ æ ‡ç‚¹ç¬¦å·

### ğŸ—ï¸ æ¨¡å‹æ¶æ„

#### çŸ¥è¯†è’¸é¦ï¼ˆTeacher-Student æ¨¡å¼ï¼‰

è’¸é¦è¿‡ç¨‹éµå¾ªæ•™å¸ˆ-å­¦ç”ŸèŒƒå¼ï¼š

- **æ•™å¸ˆæ¨¡å‹**ï¼šWhisper-smallï¼ˆ12 å±‚ç¼–ç å™¨ + 12 å±‚è§£ç å™¨ï¼‰
- **å­¦ç”Ÿæ¨¡å‹**ï¼šDistil-Whisper-smallï¼ˆ12 å±‚ç¼–ç å™¨ + 2 å±‚è§£ç å™¨ï¼‰
- **åˆå§‹åŒ–ç­–ç•¥**ï¼šä»æ•™å¸ˆæ¨¡å‹æœ€å¤§é—´éš”å±‚å¤åˆ¶æƒé‡åˆ°å­¦ç”Ÿæ¨¡å‹
- **è®­ç»ƒç­–ç•¥**ï¼šå†»ç»“ç¼–ç å™¨ï¼Œä»…è®­ç»ƒè§£ç å™¨ä»¥ä¼˜åŒ–æ•ˆç‡

#### æŸå¤±å‡½æ•°

- **äº¤å‰ç†µæŸå¤±**ï¼šè¡¡é‡é¢„æµ‹å‡†ç¡®åº¦ä¸çœŸå®æ ‡ç­¾çš„å·®å¼‚
- **KL æ•£åº¦æŸå¤±**ï¼šæ•è·ä»æ•™å¸ˆåˆ°å­¦ç”Ÿçš„çŸ¥è¯†è½¬ç§»
- **ç»„åˆæŸå¤±**ï¼šåŠ æƒç»„åˆï¼ˆ0.8 Ã— äº¤å‰ç†µ + KL æƒé‡ Ã— KL æ•£åº¦ï¼‰

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### ç¯å¢ƒé…ç½®

1. **å®‰è£…ä¾èµ–**
   ```bash
   pip install -e .
   ```

2. **é…ç½® Accelerate**
   ```bash
   accelerate config
   ```

3. **Hugging Face èº«ä»½éªŒè¯**
   ```bash
   git config --global credential.helper store
   huggingface-cli login
   ```

#### ä¼ªæ ‡ç­¾æå–

ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆä¼ªæ ‡ç­¾ï¼š

```bash
bash run_pseudo_labelling.sh
```

**æ³¨æ„**ï¼šè¿è¡Œå‰è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ `model_name_or_path`ã€`dataset_name` ç­‰å‚æ•°ã€‚

#### æ¨¡å‹åˆå§‹åŒ–

åˆ›å»ºå¹¶åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹ï¼š

```bash
python create_student_model.py \
  --teacher_checkpoint "openai/whisper-small" \
  --encoder_layers 12 \
  --decoder_layers 2 \
  --save_dir "./distil-small-init"
```

åˆå§‹åŒ–çš„å­¦ç”Ÿæ¨¡å‹å°†ä¿å­˜åˆ° `./distil-small-init`ã€‚

#### è’¸é¦è®­ç»ƒ

æ‰§è¡Œè’¸é¦è®­ç»ƒè„šæœ¬ï¼š

```bash
bash run_distillation.sh
```

è®­ç»ƒå®Œæˆçš„æ¨¡å‹å°†ä¿å­˜åˆ° `./model`ã€‚åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œç¼–ç å™¨è¢«å†»ç»“ï¼Œä»…è®­ç»ƒè§£ç å™¨ï¼ˆé€šè¿‡ `freeze_encoder` å‚æ•°é…ç½®ï¼‰ã€‚

#### æ¨¡å‹è¯„ä¼°

è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š

```bash
bash run_eval_sf.sh
```

è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š
- **CER**ï¼ˆå­—ç¬¦é”™è¯¯ç‡ï¼‰ï¼šè¡¡é‡å­—ç¬¦çº§åˆ«çš„è¯†åˆ«å‡†ç¡®åº¦
- **RTF**ï¼ˆåå®æ—¶å› å­ï¼‰ï¼šéŸ³é¢‘æ—¶é•¿ä¸å¤„ç†æ—¶é—´çš„æ¯”å€¼

### ğŸ“Š å®éªŒç»“æœ

#### æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | CERï¼ˆå¹³å‡ï¼‰ | é€Ÿåº¦æå‡ | æ•°æ®é›† |
|------|--------|------------|---------|--------|
| Whisper-small | 230.54M | 17.43% | 1.0x | Common Voice + FLEURS + AISHELL |
| Distil-Whisperï¼ˆå¾®è°ƒå‰ï¼‰ | 140.41M | 27.53% | 2.26x | Common Voice + FLEURS + AISHELL |
| **Distil-Whisper-finetune** | **140.41M** | **17.11%** | **2.89x** | **Common Voice + FLEURS + AISHELL** |

#### å„æ•°æ®é›†è¯¦ç»†ç»“æœ

**Whisper-small è¡¨ç°ï¼š**

| æ•°æ®é›† | å¤§å° | CER | éªŒè¯è€—æ—¶ï¼ˆç§’ï¼‰ |
|--------|------|-----|---------------|
| Common Voice | 10,626 | 21.51% | 9,767 |
| FLEURS | 945 | 16.87% | 1,718 |
| AISHELL | 7,176 | 13.92% | 4,990 |
| **å¹³å‡** | - | **17.43%** | - |

**Distil-Whisper-finetune è¡¨ç°ï¼š**

| æ•°æ®é›† | å¤§å° | CER | CER å˜åŒ– | éªŒè¯è€—æ—¶ï¼ˆç§’ï¼‰ | è€—æ—¶ä¸‹é™ | é€Ÿåº¦æå‡ |
|--------|------|-----|---------|---------------|---------|---------|
| Common Voice | 10,626 | 18.73% | â†“12.93% | 2,511 | 74.29% | 2.89x |
| FLEURS | 945 | 24.54% | â†‘45.51% | 459 | 73.28% | 2.74x |
| AISHELL | 7,176 | 8.06% | â†“42.06% | 1,240 | 76.53% | 3.02x |
| **å¹³å‡** | - | **17.11%** | **â†“3.16%** | - | **74.70%** | **2.89x** |

*éªŒè¯è€—æ—¶åœ¨ 2Ã—NVIDIA T4 GPU ä¸Šæµ‹é‡ï¼Œbatch_size=64*

#### ä¸»è¦å‘ç°

1. **æ¨¡å‹å‹ç¼©**ï¼šæˆåŠŸå°†æ¨¡å‹è§„æ¨¡å‡å° 39.09%ï¼ˆå‡å°‘ 9000 ä¸‡å‚æ•°ï¼‰
2. **æ¨ç†é€Ÿåº¦**ï¼šæ¨ç†æ—¶é—´æå‡ 2.89 å€
3. **è¯†åˆ«å‡†ç¡®åº¦**ï¼šç›¸æ¯”æ•™å¸ˆæ¨¡å‹ï¼ŒCER å¹³å‡é™ä½ 3.16%
4. **è¯­è¨€é€‚åº”**ï¼šå¾®è°ƒåçš„æ¨¡å‹åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºåŸå§‹å¤šè¯­è¨€æ¨¡å‹
5. **æ ‡ç‚¹ç¬¦å·è¯†åˆ«**ï¼šå¢å¼ºäº†å¯¹æ ‡ç‚¹ç¬¦å·çš„è¯†åˆ«å’Œè¾“å‡ºèƒ½åŠ›
6. **ç®€ä½“ä¸­æ–‡ç›´æ¥è¾“å‡º**ï¼šæ— éœ€è¿›è¡Œç¹ç®€è½¬æ¢

### ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹

#### æŸå¤±æ›²çº¿

**è®­ç»ƒæŸå¤±å˜åŒ–ï¼š**
- **åˆå§‹é˜¶æ®µ**ï¼ˆ0-1000 æ­¥ï¼‰ï¼šæŸå¤±å¿«é€Ÿä¸‹é™ï¼Œæ¨¡å‹å­¦ä¹ åŸºç¡€æ¨¡å¼
- **ä¸­é—´é˜¶æ®µ**ï¼ˆ1000-2000 æ­¥ï¼‰ï¼šæŸå¤±é€æ¸å‡å°ï¼Œæ”¶æ•›é€Ÿåº¦æ”¾ç¼“
- **æ”¶æ•›é˜¶æ®µ**ï¼ˆ2000+ æ­¥ï¼‰ï¼šæŸå¤±ç¨³å®šåœ¨ 0.2 å·¦å³ï¼Œè¾¾åˆ°æœ€ä¼˜è®­ç»ƒçŠ¶æ€

**å¾®è°ƒæŸå¤±ï¼š**
- åˆå§‹å¿«é€Ÿä¸‹é™ï¼Œä»çº¦ 0.55 é™è‡³çº¦ 0.35
- åœ¨ 10,000 æ­¥å†…ç¨³æ­¥ä¸‹é™è‡³çº¦ 0.2
- æœ€ç»ˆæ”¶æ•›è¡¨æ˜æˆåŠŸé€‚åº”ä¸­æ–‡æ•°æ®é›†

### ğŸ› ï¸ å…¶ä»–å·¥å…·

- **`test_whisper.py`**ï¼šåœ¨åœ¨çº¿æ•°æ®é›†ä¸Šè¿›è¡Œæ¨ç†
- **`test_whisper_local.py`**ï¼šåœ¨æœ¬åœ° WAV éŸ³é¢‘æ–‡ä»¶ä¸Šè¿›è¡Œæ¨ç†
- **`count_params.py`**ï¼šè®¡ç®—æ¨¡å‹å‚æ•°é‡

### ğŸ“ é¡¹ç›®ç»“æ„

```
Distil-whisper-small-zh/
â”œâ”€â”€ cer/                          # å­—ç¬¦é”™è¯¯ç‡è¯„ä¼°æ¨¡å—
â”‚   â”œâ”€â”€ cer.py                    # CER æŒ‡æ ‡å®ç°
â”‚   â””â”€â”€ data_utils.py             # CER è®¡ç®—æ•°æ®å·¥å…·
â”œâ”€â”€ create_student_model.py       # å­¦ç”Ÿæ¨¡å‹åˆå§‹åŒ–
â”œâ”€â”€ run_distillation.py           # ä¸»è’¸é¦è®­ç»ƒè„šæœ¬
â”œâ”€â”€ run_distillation.sh           # è’¸é¦è®­ç»ƒ Shell è„šæœ¬
â”œâ”€â”€ run_pseudo_labelling.py       # ä¼ªæ ‡ç­¾ç”Ÿæˆ
â”œâ”€â”€ run_pseudo_labelling.sh       # ä¼ªæ ‡ç­¾ Shell è„šæœ¬
â”œâ”€â”€ run_eval.py                   # æ¨¡å‹è¯„ä¼°è„šæœ¬
â”œâ”€â”€ run_eval_sf.sh                # è¯„ä¼° Shell è„šæœ¬
â”œâ”€â”€ test_whisper.py               # åœ¨çº¿æ•°æ®é›†æ¨ç†
â”œâ”€â”€ test_whisper_local.py         # æœ¬åœ°éŸ³é¢‘æ–‡ä»¶æ¨ç†
â”œâ”€â”€ count_params.py               # å‚æ•°è®¡æ•°å·¥å…·
â”œâ”€â”€ data_utils.py                 # æ•°æ®å¤„ç†å·¥å…·
â”œâ”€â”€ setup.py                      # åŒ…é…ç½®æ–‡ä»¶
â””â”€â”€ README.md                     # é¡¹ç›®æ–‡æ¡£
```

### ğŸ”¬ ç ”ç©¶æ–¹æ³•

#### ä¸‰é˜¶æ®µå®éªŒè®¾è®¡

**é˜¶æ®µä¸€ï¼šè’¸é¦**
- ä»æ•™å¸ˆæ¨¡å‹æå–ä¼ªæ ‡ç­¾
- é€šè¿‡æœ€å¤§é—´éš”å±‚å¤åˆ¶åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹
- ä½¿ç”¨äº¤å‰ç†µå’Œ KL æ•£åº¦ç»„åˆæŸå¤±è®­ç»ƒ
- å†»ç»“ç¼–ç å™¨ï¼Œä»…è®­ç»ƒè§£ç å™¨

**é˜¶æ®µäºŒï¼šå¾®è°ƒ**
- åº”ç”¨ AdaLoRA è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
- åœ¨ Common Voiceã€FLEURS å’Œ AISHELL ç»„åˆæ•°æ®é›†ä¸Šè®­ç»ƒ
- å¢å¼ºä¸­æ–‡è¯­è¨€é€‚åº”èƒ½åŠ›
- æ”¹å–„æ ‡ç‚¹ç¬¦å·è¯†åˆ«

**é˜¶æ®µä¸‰ï¼šè¯„ä¼°**
- åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šæµ‹è¯•
- ä¸åŸºçº¿æ¨¡å‹ï¼ˆWhisper-baseã€Whisper-tinyï¼‰å¯¹æ¯”
- æµ‹é‡ CER å’Œ RTF æŒ‡æ ‡
- å¯¹çœŸå®å½•éŸ³è¿›è¡Œå®šæ€§åˆ†æ

### ğŸ“ ç ”ç©¶èƒŒæ™¯ä¸ç›¸å…³å·¥ä½œ

#### Whisper æ¨¡å‹ä»‹ç»
Whisper æ˜¯ OpenAI å¼€å‘çš„å…ˆè¿›è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œåœ¨è‹±æ–‡ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œå…·æœ‰å‡ºè‰²çš„é€Ÿåº¦å’Œç›¸å¯¹è¾ƒå°çš„æ¨¡å‹ä½“ç§¯ã€‚æœ¬é¡¹ç›®åŸºäº Whisper-small æ¨¡å‹ï¼Œé€šè¿‡çŸ¥è¯†è’¸é¦æŠ€æœ¯å®ç°æ¨¡å‹å‹ç¼©å’Œä¼˜åŒ–ã€‚

#### çŸ¥è¯†è’¸é¦åœ¨ NLP é¢†åŸŸçš„åº”ç”¨
çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§å°†å¤§å‹æ¨¡å‹çŸ¥è¯†è½¬ç§»ç»™å°å‹æ¨¡å‹çš„æŠ€æœ¯ï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå·²å±•ç°å‡ºæ˜¾è‘—æ½œåŠ›ã€‚é€šè¿‡å°†æ•™å¸ˆæ¨¡å‹çš„çŸ¥è¯†è’¸é¦åˆ°å­¦ç”Ÿæ¨¡å‹ï¼Œå¯ä»¥åœ¨æ€§èƒ½å‡ ä¹ä¸å—å½±å“çš„æƒ…å†µä¸‹æ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å’Œè®¡ç®—éœ€æ±‚ã€‚

#### ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”

æœ¬é¡¹ç›®é€‰æ‹© Whisper-small ä½œä¸ºæ•™å¸ˆæ¨¡å‹çš„åŸå› ï¼š
- **Whisper-small**ï¼š12 å±‚ç¼–ç å™¨ + 12 å±‚è§£ç å™¨
- **Whisper-base**ï¼š6 å±‚ç¼–ç å™¨ + 6 å±‚è§£ç å™¨  
- **Whisper-tiny**ï¼š4 å±‚ç¼–ç å™¨ + 4 å±‚è§£ç å™¨

Whisper-base å’Œ Whisper-tiny çš„æ¨¡å‹ç»“æ„ç›¸å¯¹ç®€å•ï¼ŒçŸ¥è¯†è’¸é¦å¯¹å…¶å‹ç¼©æ•ˆæœæœ‰é™ï¼Œå› æ­¤é€‰æ‹©ç»“æ„æ›´å¤æ‚çš„ Whisper-small è¿›è¡Œè’¸é¦ã€‚

### ğŸ“– å‚è€ƒæ–‡çŒ®

1. Radford, A., et al. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. arXiv preprint arXiv:2212.04356.
2. Sanh, V., et al. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.
3. Devlin, J., et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.
4. Hugging Face. (2021). Distil-Whisper: Distilling OpenAI's Whisper for Faster, Smaller Models.

### ğŸ™ è‡´è°¢

æ„Ÿè°¢ OpenAI æä¾› Whisper æ¨¡å‹ï¼Œæ„Ÿè°¢ Hugging Face æä¾› Distil-Whisper æ¡†æ¶å’Œå·¥å…·æ”¯æŒã€‚

---

<div align="center">

**å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿ Star â­**

Made with â¤ï¸ by Team 12

</div>
